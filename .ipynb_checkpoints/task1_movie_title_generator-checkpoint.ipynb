{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from opencc import OpenCC\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from collections import Counter\n",
    "import random\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import word2vec\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "import time\n",
    "import jieba\n",
    "jieba.set_dictionary('/Users/Wan-Ting/Google Drive/NCTU/NLP/finalproj/others_work/dict.txt.big1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t2s = OpenCC('t2s')\n",
    "s2t = OpenCC('s2t')\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "nlpw2v = word2vec.Word2Vec.load('/Users/Wan-Ting/Google Drive/NCTU/NLP/finalproj/others_work/word2vec/w2v.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_time_stamp(l):\n",
    "    if l[:2].isnumeric() and l[2] == ':':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def has_no_text(line):\n",
    "    #移除分隔符號\n",
    "    l = line.strip('\\n')\n",
    "    if not len(l):\n",
    "        return True\n",
    "    if l.isnumeric():\n",
    "        return True\n",
    "    if is_time_stamp(l):\n",
    "        return True\n",
    "    if l[0] == '(' and l[-1] == ')':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_lowercase_letter_or_comma(letter):\n",
    "    if letter.isalpha() and letter.lower() == letter:\n",
    "        return True\n",
    "    if letter == ',':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clean_up(lines):\n",
    "    new_lines = []\n",
    "    for line in lines[:]:\n",
    "        #line = lines[6]\n",
    "        if has_no_text(line):\n",
    "            continue\n",
    "        else:\n",
    "          #append line\n",
    "          new_lines.append(line)\n",
    "    return new_lines    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '?', '、', '。', '“', '”', '《', '》', '！', '，']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/Wan-Ting/Google Drive/NCTU/NLP/finalproj/stopword.txt','r') as f:\n",
    "    stopword = f.read()\n",
    "stopword = stopword.split(\"\\n\")\n",
    "stopword[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_stopword(data):\n",
    "    data_list=[]\n",
    "    for word in data:\n",
    "        if word not in stopword:\n",
    "            data_list.append(word)\n",
    "    return data_list\n",
    "\n",
    "def time_transform(s):\n",
    "    if s == '今天':\n",
    "        s = '今日'\n",
    "    if s == '明天':\n",
    "        s = '明日'\n",
    "    return s        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(scriptfile,root='/Users/Wan-Ting/Downloads/input'):\n",
    "    s = '\"\"‘“『』「」[]...*◎#＃\\n— \\t<i></i><I></I><B></B>{\\an8}\\{1c H0080FF}wwwgamedsc{i1}i{pos 190 200.104}﹒∮—-abcdefghijklmnopqrstuvwxyz'\n",
    "    file_name = scriptfile\n",
    "    with open(os.path.join(root,file_name), encoding='utf-8', errors='replace') as f:\n",
    "        lines = f.readlines()\n",
    "        new_lines = clean_up(lines)\n",
    "    movie_lines = []\n",
    "    for line in new_lines:\n",
    "        movie_lines.append(line.translate(str.maketrans('','',s)))\n",
    "    return movie_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jieba_seg(scriptfile):\n",
    "    lines = read_data(scriptfile,root='/Users/Wan-Ting/Downloads/input')\n",
    "    seg_movie = []\n",
    "    for i in lines:\n",
    "        re = []\n",
    "        words = jieba.cut(i, cut_all=False)\n",
    "        for word in words:\n",
    "            if word not in stopword:\n",
    "                re.append(word)\n",
    "        seg_movie.append(re)\n",
    "    flat_list = [item for sublist in seg_movie for item in sublist]\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_script(movie_lines):\n",
    "    tstart = time.time()\n",
    "    # annotation\n",
    "    data = {}\n",
    "    for i,line in enumerate(movie_lines):\n",
    "        d = nlp.annotate(t2s.convert(line), properties={\n",
    "          'annotators': 'tokenize,ssplit,pos,ner',\n",
    "          'outputFormat': 'json'\n",
    "          })\n",
    "        data['%d'%i] = d\n",
    "    \n",
    "    # extract data\n",
    "    information = []\n",
    "    for i in range(len(data)):\n",
    "        try:\n",
    "            sentences = data['%d'%i]['sentences']\n",
    "            tuple_list = []\n",
    "            for j in range(len(sentences)):\n",
    "                tokens = sentences[j]['tokens']\n",
    "                for k in range(len(tokens)):\n",
    "                    s_tuple = (tokens[k]['lemma'],tokens[k]['pos'],tokens[k]['ner'])\n",
    "                    tuple_list.append(s_tuple)\n",
    "            information.append(tuple_list)\n",
    "        except:\n",
    "            information.append('error')\n",
    "    information = [x for x in information if x!='error']\n",
    "    print('PARSING TIME :%.2f secs' %(time.time() - tstart))   \n",
    "    return information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_similar(term,nlpw2v):\n",
    "    try:\n",
    "        similar_term = nlpw2v.most_similar(s2t.convert(term), topn=1)[0][0]\n",
    "    except:\n",
    "        similar_term = term\n",
    "    return similar_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def title_collocation(data):\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(data)\n",
    "    finder.apply_freq_filter(3)\n",
    "    i = 0\n",
    "    if len(finder.nbest(bigram_measures.likelihood_ratio, 10))!=0:\n",
    "        while i<len(finder.nbest(bigram_measures.likelihood_ratio, 10)):\n",
    "            tpl = finder.nbest(bigram_measures.likelihood_ratio, 10)[i]\n",
    "            if tpl[0]==tpl[1]:\n",
    "                i=i+1\n",
    "            else:\n",
    "                break\n",
    "    else:\n",
    "        tpl ='NO MATCH FOUND'\n",
    "    title1=''.join(tpl)\n",
    "    title1=s2t.convert(title1)\n",
    "    #try:\n",
    "    #    a1_t = change_similar(tpl[0],nlpw2v)\n",
    "    #    a2_t = change_similar(tpl[1],nlpw2v)\n",
    "    #    title2 = a1_t + a2_t\n",
    "    #except:\n",
    "    #    title2 = title1\n",
    "    return title1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def title_frequency(information):\n",
    "    # filter ner==person or pos not NNVVNT\n",
    "    tuple_bag = [t for sent in information for t in sent]\n",
    "    tuple_candidate = []\n",
    "    for t in tuple_bag:\n",
    "        if ((((t[0] not in stopword) and t[2]!='PERSON')  and t[1] in ['NN','VV','NT']) and len(t[0]) >= 2):\n",
    "            tuple_candidate.append(t)\n",
    "    tuple_candidate = (Counter(tuple_candidate)).most_common()\n",
    "    \n",
    "    # rule\n",
    "    d = [tuple_candidate[0:5][i][0][2] for i in range(0,5)]\n",
    "    l_t = len(tuple_candidate)\n",
    "    d_all = [tuple_candidate[0:l_t][i] for i in range(0,l_t) if tuple_candidate[0:l_t][i][0][2] == 'DATE']\n",
    "    if 'DATE' in d:\n",
    "        a1 = tuple_candidate[d.index('DATE')][0][0]\n",
    "        a1 = time_transform(a1)\n",
    "        a2 = [x for x in tuple_candidate if x not in d_all][0][0][0]\n",
    "    else:\n",
    "        if tuple_candidate[0][0][1] == 'NN':\n",
    "            a1 = tuple_candidate[1][0][0]\n",
    "            a2 = tuple_candidate[0][0][0]\n",
    "        else:\n",
    "            n_all = [tuple_candidate[0:l_t][i] for i in range(0,l_t) if tuple_candidate[0:l_t][i][0][1] == 'NN']\n",
    "            a1 = tuple_candidate[0][0][0]\n",
    "            a2 = n_all[0][0][0]\n",
    "    title1 = s2t.convert(a1+a2)\n",
    "    return title1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def title_generator(scriptfile):\n",
    "    data = jieba_seg(scriptfile)\n",
    "    title = title_collocation(data)\n",
    "    if title =='NO MATCH FOUND':\n",
    "        lines = read_data(scriptfile,root='/Users/Wan-Ting/Downloads/input')\n",
    "        information=extract_script(lines)\n",
    "        print('method2:frequency')\n",
    "        title = title_frequency(information)\n",
    "    else:\n",
    "        print('method1:collocation')\n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mvls為想生成電影名稱的字幕檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Wan-Ting/Google Drive/NCTU/NLP/finalproj'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mvls = [str(i+1)+'.srt' for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/Wan-Ting/Google Drive/NCTU/NLP/finalproj/others_work/dict.txt.big1.txt ...\n",
      "Dumping model to file cache /var/folders/q2/_z14v11n4hl9pw308x7_qs_80000gn/T/jieba.ud9d39d53952634e7a1e9fd07ad823249.cache\n",
      "Loading model cost 3.523 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method1:collocation\n",
      "method1:collocation\n",
      "method1:collocation\n",
      "method1:collocation\n",
      "method1:collocation\n",
      "method1:collocation\n",
      "method1:collocation\n",
      "method1:collocation\n",
      "method1:collocation\n",
      "method1:collocation\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "create = open ('/Users/Wan-Ting/Google Drive/NCTU/NLP/finalproj/task1_group5.txt','w')\n",
    "#mvls = os.listdir('/Users/Wan-Ting/Downloads/input')\n",
    "for scriptfile in mvls:\n",
    "    try:\n",
    "        title = title_generator(scriptfile)\n",
    "        create.write(scriptfile.split('.srt')[0]+'\\t'+title+'\\n')\n",
    "    except:\n",
    "        print(scriptfile)\n",
    "create.close()\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
